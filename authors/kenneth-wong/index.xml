<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kenneth Wong | Kenneth</title>
    <link>https://kennethwong.tech/authors/kenneth-wong/</link>
      <atom:link href="https://kennethwong.tech/authors/kenneth-wong/index.xml" rel="self" type="application/rss+xml" />
    <description>Kenneth Wong</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>zh-Hans</language><copyright>© 2021 All rights reserved.</copyright><lastBuildDate>Sun, 27 Jun 2021 21:57:34 +0800</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Kenneth Wong</title>
      <link>https://kennethwong.tech/authors/kenneth-wong/</link>
    </image>
    
    <item>
      <title>Swin Transformer 解析</title>
      <link>https://kennethwong.tech/post/swin_transformer/</link>
      <pubDate>Sun, 27 Jun 2021 21:57:34 +0800</pubDate>
      <guid>https://kennethwong.tech/post/swin_transformer/</guid>
      <description>&lt;h3 id=&#34;swin-transformer1&#34;&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2103.14030v1.pdf&#34; title=&#34;Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, arXiv 2021.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Swin-Transformer&lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;Transformer在NLP领域被广泛应用，能够建立大范围的数据之间的依赖关系，以attention的形式。语言具有良好的单个词作为元素基础，但是图像的基本元素在尺度变化上是非常大的，现有的基于transformer的视觉模型的token尺度都是固定的，对于视觉任务并不适合。另一方面，视觉任务具有更高的分辨率要求，比如semantic segmentation，精确到像素级，并不适合直接利用transformer的self attention机制（将是image size的平方了量级）。而Swin Transformer构建层级的feature map，而且计算复杂度是image size的线性函数。整体来说，Swin Transformer先由小patch开始，再到深层融合邻居patch信息。并且self-attention只用在不相邻的大窗口（就是shift window，Swin的全称）内，窗口之间并没有overlap。&lt;/p&gt;
&lt;div align=center&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/swin_transformer/swin.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Swin-Transformer作为一个适合与视觉任务，特别是适合稠密预测（检测、分割）的backbone而出现，它可以结合众多的检测方法。在此之前，有
&lt;a href=&#34;https://arxiv.org/pdf/2012.12877.pdf&#34; title=&#34;Training data-efficient image transformers and distillation through attention, 2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ViT&lt;/a&gt;
以及它的改进工作提出的backbone，ViT需要大量数据预训练（JFT-300M），它的改进[DeiT][3]运用了一些训练策略使得可以在ImageNet-1K上预训练。ViT虽然取得令人振奋的效果，但是它本身其实并不适合作为密集视觉任务的backbone，因为它的低分辨率的特征图与平方计算复杂度。&lt;/p&gt;
&lt;h4 id=&#34;细节过程&#34;&gt;细节过程&lt;/h4&gt;
&lt;div align=center&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/swin_transformer/swin_frwork.png&#34; alt=&#34;Swin Transformer基本结构&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;这里结合代码研究分析Swin Transformer的具体过程和原理。先给出整个流程的定义。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class SwinTransformer(nn.Module):
    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,
                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],
                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False, **kwargs):
        super().__init__()

        self.num_classes = num_classes 
        self.num_layers = len(depths)  ## 4
        self.embed_dim = embed_dim  # 96
        self.ape = ape  # the result is bad
        self.patch_norm = patch_norm  
        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))  # 96 * 2^3
        self.mlp_ratio = mlp_ratio

        # split image into non-overlapping patches
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,
            norm_layer=norm_layer if self.patch_norm else None)
        num_patches = self.patch_embed.num_patches  # 56*56
        patches_resolution = self.patch_embed.patches_resolution  # (56, 56)
        self.patches_resolution = patches_resolution

        # absolute position embedding
        if self.ape:
            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
            trunc_normal_(self.absolute_pos_embed, std=.02)

        self.pos_drop = nn.Dropout(p=drop_rate)

        # stochastic depth
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule

        # build layers
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),
                               input_resolution=(patches_resolution[0] // (2 ** i_layer),
                                                 patches_resolution[1] // (2 ** i_layer)),
                               depth=depths[i_layer],
                               num_heads=num_heads[i_layer],
                               window_size=window_size,
                               mlp_ratio=self.mlp_ratio,
                               qkv_bias=qkv_bias, qk_scale=qk_scale,
                               drop=drop_rate, attn_drop=attn_drop_rate,
                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
                               norm_layer=norm_layer,
                               downsample=PatchMerging if (i_layer &amp;lt; self.num_layers - 1) else None,
                               use_checkpoint=use_checkpoint)
            self.layers.append(layer)

        self.norm = norm_layer(self.num_features)
        self.avgpool = nn.AdaptiveAvgPool1d(1)
        self.head = nn.Linear(self.num_features, num_classes) if num_classes &amp;gt; 0 else nn.Identity()

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {&#39;absolute_pos_embed&#39;}

    @torch.jit.ignore
    def no_weight_decay_keywords(self):
        return {&#39;relative_position_bias_table&#39;}

    def forward_features(self, x):
        x = self.patch_embed(x)
        if self.ape:
            x = x + self.absolute_pos_embed
        x = self.pos_drop(x)

        for layer in self.layers:
            x = layer(x)

        x = self.norm(x)  # B L C
        x = self.avgpool(x.transpose(1, 2))  # B C 1
        x = torch.flatten(x, 1)
        return x

    def forward(self, x):
        x = self.forward_features(x)
        x = self.head(x)
        return x
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;预处理&#34;&gt;预处理&lt;/h5&gt;
&lt;p&gt;对于分类模型，输入图像尺寸为$224\times 224 \times 3$，即$H=W=224$。按照原文描述，模型先将图像分割成每块大小为$4\times 4$的patch，那么就会有$56\times 56$个patch，这就是初始resolution，也是后面每个stage会降采样的维度。后面每个stage都会降采样时长宽降到一半，特征数加倍。按照原文及原图描述，划分的每个patch具有$4\times4\times3=48$维特征。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;实际在代码中，首先使用了PatchEmbed模块，定义如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class PatchEmbed(nn.Module):
    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
        self.img_size = img_size
        self.patch_size = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1]
  
        self.in_chans = in_chans
        self.embed_dim = embed_dim
  
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None
  
    def forward(self, x):
        B, C, H, W = x.shape
        # FIXME look at relaxing size constraints
        assert H == self.img_size[0] and W == self.img_size[1], \
            f&amp;quot;Input image size ({H}*{W}) doesn&#39;t match model ({self.img_size[0]}*{self.img_size[1]}).&amp;quot;
        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C
        if self.norm is not None:
            x = self.norm(x)
        return x
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以看到，实际操作使用了一个卷积层conv2d(3, 96, 4, 4)，直接就做了划分patch和编码初始特征的工作，对于输入$x: B\times 3\times 224\times 224$，经过一层conv2d和LayerNorm得到$x: B\times 56^2\times 96$。然后作为对比，可以选择性地加上每个patch的绝对位置编码，原文实验表示这种做法不好，因此不会采用（ape=false）。最后经过一层dropout，至此，预处理完成。另外，要注意的是，代码和上面流程图并不符，其实在stage 1之前，即预处理完成后，维度已经是$H/4\times W/4\times C$，stage 1之后已经是$H/8\times W/8\times 2C$，不过在stage 4后不再降采样，得到的还是$H/32\times W/32 \times 8C$。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;stage处理&#34;&gt;stage处理&lt;/h5&gt;
&lt;p&gt;我们先梳理整个stage的大体过程，把简单的部分先说了，再深入到复杂得的细节。每个stage，即代码中的BasicLayer，由若干个block组成，而block的数目由depth列表中的元素决定。每个block就是W-MSA（window-multihead self attention）或者SW-MSA（shift window multihead self attention），一般有偶数个block，两种SA交替出现，比如6个block，0，2，4是W-MSA，1，3，5是SW-MSA。在经历完一个stage后，会进行下采样，定义的下采样比较有意思。比如还是$56\times 56$个patch，四个为一组，分别取每组中的左上，右上、左下、右下堆叠一起，经过一个layernorm，linear层，实现维度下采样、特征加倍的效果。实际上它可以看成一种加权池化的过程。代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class PatchMerging(nn.Module):
    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
        super().__init__()
        self.input_resolution = input_resolution
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = norm_layer(4 * dim)

    def forward(self, x):
        &amp;quot;&amp;quot;&amp;quot;
        x: B, H*W, C
        &amp;quot;&amp;quot;&amp;quot;
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, &amp;quot;input feature has wrong size&amp;quot;
        assert H % 2 == 0 and W % 2 == 0, f&amp;quot;x size ({H}*{W}) are not even.&amp;quot;

        x = x.view(B, H, W, C)

        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C

        x = self.norm(x)
        x = self.reduction(x)

        return x
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在经历完4个stage后，得到的是$(H/32\times W/32)\times 8C$的特征，将其转到$8C\times (H/32\times W/32)$后，接一个AdaptiveAvgPool1d(1)，全局平均池化，得到$8C$特征，最后接一个分类器。&lt;/p&gt;
&lt;h5 id=&#34;block处理&#34;&gt;Block处理&lt;/h5&gt;
&lt;p&gt;上面说到有两种block，block的代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class SwinTransformerBlock(nn.Module):
    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        if min(self.input_resolution) &amp;lt;= self.window_size:
            # if window size is larger than input resolution, we don&#39;t partition windows
            self.shift_size = 0
            self.window_size = min(self.input_resolution)
        assert 0 &amp;lt;= self.shift_size &amp;lt; self.window_size, &amp;quot;shift_size must in 0-window_size&amp;quot;

        self.norm1 = norm_layer(dim)
        self.attn = WindowAttention(
            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,
            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)

        self.drop_path = DropPath(drop_path) if drop_path &amp;gt; 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

        if self.shift_size &amp;gt; 0:
            # calculate attention mask for SW-MSA
            H, W = self.input_resolution
            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1
            h_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            w_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            cnt = 0
            for h in h_slices:
                for w in w_slices:
                    img_mask[:, h, w, :] = cnt
                    cnt += 1

            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1
            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
        else:
            attn_mask = None

        self.register_buffer(&amp;quot;attn_mask&amp;quot;, attn_mask)

    def forward(self, x):
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, &amp;quot;input feature has wrong size&amp;quot;

        shortcut = x
        x = self.norm1(x)
        x = x.view(B, H, W, C)

        # cyclic shift
        if self.shift_size &amp;gt; 0:
            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
        else:
            shifted_x = x

        # partition windows
        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C

        # W-MSA/SW-MSA
        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C

        # merge windows
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H&#39; W&#39; C

        # reverse cyclic shift
        if self.shift_size &amp;gt; 0:
            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
        else:
            x = shifted_x
        x = x.view(B, H * W, C)

        # FFN
        x = shortcut + self.drop_path(x)
        x = x + self.drop_path(self.mlp(self.norm2(x)))

        return x
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;w-msa&#34;&gt;W-MSA&lt;/h6&gt;
&lt;p&gt;W-MSA比较简单，只要其中&lt;code&gt;shift_size&lt;/code&gt;设置为0就是W-MSA。下面跟着代码走一遍过程。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;输入：$x: B\times 56^2\times 96$，$H, W=56$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;经过一层layerNorm&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;变形：$x: B\times 56\times 56\times 96$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;直接赋值给&lt;code&gt;shifted_x&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;调用&lt;code&gt;window_partition&lt;/code&gt;函数，输入&lt;code&gt;shifted_x&lt;/code&gt;，&lt;code&gt;window_size=7&lt;/code&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;注意窗口大小以patch为单位，比如7就是7个patch，如果56的分辨率就会有8个窗口。&lt;/li&gt;
&lt;li&gt;这个函数对&lt;code&gt;shifted_x&lt;/code&gt;做一系列变形，最终变成$8^2B\times 7 \times 7\times 96$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;返回赋值给&lt;code&gt;x_windows&lt;/code&gt;，再变形成$8^2B\times 7^2\times 96$，这表示所有图片，每个图片的64个window，每个window内有49个patch。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;调用&lt;code&gt;WindowAttention&lt;/code&gt;层，这里以它的&lt;code&gt;num_head&lt;/code&gt;为3为例。输入参数为&lt;code&gt;x_windows&lt;/code&gt;和&lt;code&gt;self.attn_mask&lt;/code&gt;，对于W-MSA，&lt;code&gt;attn_mask&lt;/code&gt;为None，可以不用管。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;WindowAttention&lt;/code&gt;代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class WindowAttention(nn.Module):
    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):
    
        super().__init__()
        self.dim = dim
        self.window_size = window_size  # Wh, Ww
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5
    
        # define a parameter table of relative position bias
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH
    
        # get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
        self.register_buffer(&amp;quot;relative_position_index&amp;quot;, relative_position_index)
    
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
    
        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)
    
    def forward(self, x, mask=None):
        &amp;quot;&amp;quot;&amp;quot;
        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
        &amp;quot;&amp;quot;&amp;quot;
        B_, N, C = x.shape
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)
    
        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))
    
        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
        attn = attn + relative_position_bias.unsqueeze(0)
    
        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)
    
        attn = self.attn_drop(attn)
    
        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;输入$x: 8^2B\times 7^2\times 96$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;产生$QKV$，调用线性层后，得到$8^2B\times 7^2\times (96\times 3)$，拆分给不同的head，得到$8^2B\times 7^2\times 3 \times 3\times 32$，第一个3是$QKV$的3，第二个3是3个head。再permute成$3\times 8^2B\times 3\times 7^2\times 32$，再拆解成$q,k,v$，每个都是$8^2B\times 3\times 7^2\times 32$。表示所有图片的每个图片64个window，每个window对应到3个不同的head，都有一套49个patch、32维的特征。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$q$归一化&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$qk$矩阵相乘求特征内积，得到$attn: 8^2B \times 3\times 7^2\times 7^2$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;得到相对位置的编码信息&lt;code&gt;relative_position_bias&lt;/code&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH
      
# get pair-wise relative position index for each token inside the window
coords_h = torch.arange(self.window_size[0])
coords_w = torch.arange(self.window_size[1])
coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
relative_coords[:, :, 1] += self.window_size[1] - 1
relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
self.register_buffer(&amp;quot;relative_position_index&amp;quot;, relative_position_index)
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这里以&lt;code&gt;window_size=3&lt;/code&gt;为例，解释以下过程：首先生成$coords: 2\times3\times 3$，就是在一个$3\times 3$的窗口内，每个位置的$y,x$坐标，而&lt;code&gt;relative_coords&lt;/code&gt;为$2\times 9 \times 9$，就是9个点中，每个点的$y$或$x$与其他所有点的差值，比如$[0]
&lt;a href=&#34;https://arxiv.org/pdf/2103.14030v1.pdf&#34; title=&#34;Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, arXiv 2021.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3&lt;/a&gt;
$，表示3号点（第二行第一个点）与1号点（第一行第二个点）的$y$坐标的差值。然后变形，并让两个坐标分别加上$3-1=2$，是因为这些坐标值范围$[0,2]$，因此差值的最小值为-2，加上2后从0开始。最后让$y$坐标乘上$2\times 3-1=5$，应该是一个trick，调整差值范围。最后将两个维度的差值相加，得到&lt;code&gt;relative_position_index&lt;/code&gt;，$3^2\times 3^2$，为9个点之间两两之间的相对位置编码值，最后用来到&lt;code&gt;self.relative_position_bias_table&lt;/code&gt;中寻址，注意相对位置的最大值为$(2M-2)(2M-1)$，而这个table最多有$(2M-1)(2M-1)$行，因此保证可以寻址，得到了一组给多个head使用的相对位置编码信息，这个table是可训练的参数。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;回到代码中，得到的&lt;code&gt;relative_position_bias&lt;/code&gt;为$3\times 7^2\times7^2$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将其加到&lt;code&gt;attn&lt;/code&gt;上，最后一个维度softmax，dropout&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;与$v$矩阵相乘，并转置，合并多个头的信息，得到$8^2B\times 7^2\times 96$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;经过一层线性层，dropout，返回&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;返回赋值给&lt;code&gt;attn_windows&lt;/code&gt;，变形为$8^2B\times 7\times 7 \times 96$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;调用&lt;code&gt;window_reverse&lt;/code&gt;，打回原状：$B\times 56\times 56\times96$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;返回给$x$，经过FFN：先加上原来的输入$x$作为residue结构，注意这里用到[timm][https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/drop.py]的&lt;code&gt;DropPath&lt;/code&gt;，并且drop的概率是整个网络结构线性增长的。然后再加上两层mlp的结果。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;返回结果$x$。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这样，整个过程就完成了，剩下的就是SW-MSA的一些不同的操作。&lt;/p&gt;
&lt;h6 id=&#34;sw-msa&#34;&gt;SW-MSA&lt;/h6&gt;
&lt;p&gt;W-MSA的不足就不同window之间是没有交流的，因此，SW-MSA紧接在W-MSA后面就是想让不同window之间也有信息传递。因此，最直接的想法就是将图片平移以下，但不是平移&lt;code&gt;window_size&lt;/code&gt;，比如平移一半，再以跟之前相同的划分来切割，这样就会有上一轮本来不在同一个window里面的pach，现在出现在同一个window。但是，最简单的处理，比如像第一个图，会出现更多的window，有些window尺寸还不一样。一种修补方法是补上一些块，padding，但是会增大计算量。为此，这里采用cycling（rolling）的方法，即滚动，将图片向左、向上平移window一半的尺寸，比如window是7，这里就平移三个patch，左边的那些块移到右边，上边的那些块移到下边。&lt;/p&gt;
&lt;p&gt;再回到block的代码中，这时&lt;code&gt;shift_size=7//2=3&lt;/code&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;输入：$x: B\times 56^2\times 96$，$H, W=56$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;经过一层layerNorm&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;变形：$x: B\times 56\times 56\times 96$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对$x$进行roll：就是分别向左向上滚动，把块补在右面和下面，赋值给&lt;code&gt;shifted_x&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;调用&lt;code&gt;window_partition&lt;/code&gt;函数，输入&lt;code&gt;shifted_x&lt;/code&gt;，&lt;code&gt;window_size=7&lt;/code&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;注意窗口大小以patch为单位，比如7就是7个patch，如果56的分辨率就会有8个窗口。&lt;/li&gt;
&lt;li&gt;这个函数对&lt;code&gt;shifted\_x&lt;/code&gt;做一系列变形，最终变成$8^2B\times 7 \times 7\times 96$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;返回赋值给&lt;code&gt;x\_windows&lt;/code&gt;，再变形成$8^2B\times 7^2\times 96$，这表示所有图片，每个图片的64个window，每个window内有49个patch。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;调用&lt;code&gt;WindowAttention&lt;/code&gt;层，这里以它的&lt;code&gt;num_head&lt;/code&gt;为3为例。输入参数为&lt;code&gt;x_windows&lt;/code&gt;和&lt;code&gt;self.attn_mask&lt;/code&gt;，对于SW-MSA，&lt;code&gt;attn_mask&lt;/code&gt;的产生过程如下。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;mask的形状为$1\times56\times56\times 1$，产生&lt;code&gt;h_slices&lt;/code&gt;和&lt;code&gt;w_slices&lt;/code&gt;都是三个：(0, -7)，(-7, -3)，(-3, None)，通过一个循环，对mask分成9大块，并打上标号。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;下图为了方便显示，换了个例子：$12\times 12$个patch，window为$3\times 3$个patch，平移量1，$4\times4$个窗口&lt;/p&gt;
&lt;div align=center&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/swin_transformer/swin_mask.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;解释：经过循环，mask被分成9大部分，分别表上0到8，然后经过&lt;code&gt;window\_partition&lt;/code&gt;，得到$4^2\times3^2$，再作差，得到$4^2\times 3^2\times3^2$，将不为0的置为-100，将为0的填上0。可以理解成，在$4^2$个窗口中的每个窗口，都有9个位置，那么这9个位置中的每个应该关注哪些位置（0），不用关注哪些位置（-100），比如右上角那个区域的左上角位置，它对应的mask就应该是上面那一个横条，不用关注右边蓝色的三格，因为右边那列是由左边平移来的，而计算两个边缘之间相关性作用不大。其他位置同理。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;那么回到原来的代码中，产生的mask就应该是$8^2\times 7^2\times7^2$，计算attention时，将$attn: 8^2B \times 3\times 7^2\times 7^2$变形成$B\times 8^2\times 3\times7^2\times7^2$加上mask，再变回去，然后softmax。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这里其实存在一点疑问：因为窗口天然隔离，上面的(0,-7), (-7, -3)这一段分割显得多余。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;后面就和W-MSA一样了，不过还要把$x$转回去，否则就要不停地转动了。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最后，Swin-Transformer有4种架构：&lt;/p&gt;
&lt;div align=center&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/swin_transformer/swin_param.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;关于MSA中的复杂度运算：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于普通的MSA，在全图上计算SA，如果尺寸为$h\times w$，那么产生$QWV$的$W$矩阵为$C\times C$，输入$x$为$hw\times C$，因此需要复杂度$3hwC^2$。然后，计算$QK^T$，$QKV$的维度是$hw\times C$，因此为$(hw)^2C$。然后softmax乘$V$得到$Z$，复杂度为$(hw)^2C$。最后，从$Z$到输出，还要乘一个矩阵$W$，复杂度$hwC^2$，因此总复杂度为$4hwC^2+2(hw)^2C$。&lt;/li&gt;
&lt;li&gt;对于W-MSA，因为分窗口进行SA，因此主要优化的是$(hw)^2C$项，现在变成$W^2\times W^2\times (h/W)\times(w/W)C=W^2hwC$，其实还是四次的，只不过将其中一个$hw$变成可以更小的$W^2$，减小的部分就是窗口与窗口之间的信息传递。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>单纯形算法</title>
      <link>https://kennethwong.tech/post/simplex/</link>
      <pubDate>Fri, 04 Jun 2021 18:12:26 +0800</pubDate>
      <guid>https://kennethwong.tech/post/simplex/</guid>
      <description>&lt;p&gt;中学课程里，我们都简单地接触过线性规划，那时候一般都是分析每个约束，在二维平面上画出直线，得到可行域，然后以固定斜率作出目标函数直线，在可行域内移动直线，在y轴上的截距就是最优解。而往往最优解的地方是通过（凸）可行域的顶点。就像下面这个例子：
$$
\begin{equation}
\begin{split}
\max&amp;amp; \quad x_3+x_4\cr
s.t.&amp;amp; \quad -x_3+2x_4\leq 2 \cr
&amp;amp;\quad 3x_3-2x_4\leq 6 \cr
&amp;amp;\quad x_3, x_4\geq 0
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;div align=center&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/img_1518533911.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;蓝色区域是可行域，红色直线是固定斜率的，当上移到（4，3）点时目标函数取最大值。&lt;/p&gt;
&lt;p&gt;而我们后面将证明，最优解一定是可行域（凸超几何体）的顶点之一。那么，我们先假定这成立，就使用”改进-停止“（improve-stopping）的套路，即给定可行域的一个顶点，求值，沿一条边到达下一个顶点，看是否能改善解，直到达到停止要求。&lt;/p&gt;
&lt;p&gt;这里就要问几个问题了：为什么最优解一定在顶点处？怎么得到顶点？怎么实现沿着一条边到下一个顶点？什么时候停止？接下来，我们将一一解答这些问题，当解答完这些问题，单纯形法也就显而易见了。&lt;/p&gt;
&lt;h2 id=&#34;1-凸多边形最优解在顶点&#34;&gt;1 凸多边形最优解在顶点&lt;/h2&gt;
&lt;p&gt;考虑最小化问题，目标函数$\mathbf{c}^T \mathbf{x}$，有一个在可行域内部的最优解$\mathbf{x}^{(0)}$，因为凸多边形内部任一点都可以表示成顶点的线性组合，即对于顶点$\mathbf{x}^{(k)}, k=1,2,\cdots, n$，有
$$
\mathbf{x}^{(0)}=\sum_{k=1}^{n}\lambda_k\mathbf{x}^{(k)},
$$&lt;/p&gt;
&lt;p&gt;$$
\sum_{k=1}^n \lambda_k=1
$$&lt;/p&gt;
&lt;p&gt;假设$\mathbf{x}^{(i)}$是所有顶点中使得$\mathbf{c}^T \mathbf{x}$最小的顶点，那么有
$$
\begin{equation}
\begin{split}
\mathbf{c}^T\mathbf{x}^{(0)} &amp;amp;= \sum_{k=1}^{n}\lambda_k\mathbf{c}^T\mathbf{x}^{(k)} \cr
&amp;amp;\geq \sum_{k=1}^{n}\lambda_k\mathbf{c}^T\mathbf{x}^{(i)} \cr
&amp;amp;= \mathbf{c}^T\mathbf{x}^{(i)}
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;因此，总有一个顶点，他的目标函数值不比内部点差。&lt;/p&gt;
&lt;h2 id=&#34;2-怎样得到一个凸多边形的顶点&#34;&gt;2 怎样得到一个凸多边形的顶点？&lt;/h2&gt;
&lt;p&gt;下面要说明的就是这样一个定理：对于可行域方程组$\mathbf{Ax=b}​$，该方程确定的凸多边形的任意一个顶点对应$\mathbf{A}​$的一组基。&lt;/p&gt;
&lt;h3 id=&#34;21-顶点对应一组基&#34;&gt;2.1 顶点对应一组基&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/img_1518534807.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;上面这个例子是松弛形式的约束，原来的变量有三个，加上后面4个后变成等式，形成的可行域如上图所示。我们取一个顶点（0，2，0）分析，代入约束中，可以算出一个完整解$x=(0，2，0，2，2，3，0)$。取出矩阵$\mathbf{A}$中对应的$x$不为0的列，即图中标蓝的几列（用$\mathbf{a}_2$，$\mathbf{a}_4$，$\mathbf{a}_5$，$\mathbf{a}_6$表示），那么这几列就是线线性无关的，考虑$m&amp;lt;n$（约束数目小于松弛后的变量个数），那么自由解有$n-m$维，因此挑出来的列必有$m$个，构成一组基。下面主要说明他们为什么线性无关。假设他们线性相关，即存在一组$\lambda\neq\mathbf{0}$，使得$\lambda_2\mathbf{a}_2+\lambda_4\mathbf{a}_4+\lambda_5\mathbf{a}_5+\lambda_6\mathbf{a}_6=0$，也就是说，可以构造$\lambda=[0, \lambda_2, 0, \lambda_4, \lambda_5, \lambda_6, 0]$，使得$\mathbf{A}\lambda=0$。那么还可以再构造两个异于$x$的解：$x&#39;=x+\theta\lambda$和$x^{&#39;&#39;}=x-\theta\lambda$。他们都满足$\mathbf{Ax=b}$。并且可以通过控制$\theta$取很小的正值，使得这两个解满足都大于0的约束。由此这两个解都在凸多面体内，并且有$x=\frac{1}{2}(x&#39;+x^{&#39;&#39;})$，但是这是有问题的，因为一个凸多面体的顶点是不能被内部点线性表示的，因此这几列是构成一组基的。&lt;/p&gt;
&lt;div align=center&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/img_1518535018.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;在这里，我们还可以对每一组解，都将$\mathbf{A}​$的列重新排列一下，将解向量也排列一下，写成分块矩阵的形式，那么就会有$\mathbf{x}_\mathbf{B}=\mathbf{B}^{-1}\mathbf{b}​$和$ \mathbf{c}^T \mathbf{x}= \mathbf{c}^T_{\mathbf{B}}\mathbf{B}^{-1}\mathbf{b}​$。这是两个很有用的式子，在后面单纯形算法的理解上很有帮助，这里先记下。&lt;/p&gt;
&lt;div align=center&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/img_1518536393.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;22-一组基对应一个基可行解顶点&#34;&gt;2.2 一组基对应一个基可行解（顶点）&lt;/h3&gt;
&lt;p&gt;有了上面的知识，给定一组基$\mathbf{B}​$，我们直接构造$\mathbf{x}=[\mathbf{B^{-1}b}, \mathbf{0}]^T​$，只要说明他不能被两个异于他的内部点线性表示即可。假设有两个内部点$\mathbf{x}_{1}​$和$\mathbf{x}_2​$，满足$\mathbf{x}=\lambda_1\mathbf{x}_1+\lambda_2\mathbf{x}_2​$，由于$\mathbf{x_N}=0​$，并且这些解的元素都大于等于0，因此$\mathbf{x}_{1\mathbf{N}}=\mathbf{x}_{2\mathbf{N}}=0​$。而又因为$\mathbf{Ax_1=Ax_2=b}​$，因此$\mathbf{x}_{1\mathbf{B}}=\mathbf{x}_{2\mathbf{B}}=\mathbf{B^{-1}b}​$。即这两个解和$\mathbf{x}​$相同，因此$\mathbf{x}​$是顶点。&lt;/p&gt;
&lt;h2 id=&#34;3-如何从一个顶点沿着边到另一个顶点&#34;&gt;3 如何从一个顶点沿着边到另一个顶点？&lt;/h2&gt;
&lt;p&gt;这里是要研究怎么改善一个解，我们需要知道怎么从一个顶点出发沿着边找到另一个顶点。前面我们知道了一个顶点对应一组基，而且一个矩阵的基有多个，那么是否可以通过基的变换从而使得顶点变换呢？先来看一个例子。&lt;/p&gt;
&lt;div align=center&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/img_1518754475.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;图中红色点对应一个完全解$\mathbf{x}=[2,0,0,2,0,3,6]$，对应的基是$\mathbf{B}=\{\mathbf{a}_1，\mathbf{a}_4，\mathbf{a}_6，\mathbf{a}_7\}$，考虑向量$\mathbf{a}_3$，即绿色那列，他可以表示成：
$$
\mathbf{a}_3=0\mathbf{a}_1+1\mathbf{a}_4+1\mathbf{a}_6+1\mathbf{a}_7
$$&lt;/p&gt;
&lt;p&gt;将式子补全，就会有
$$
0\mathbf{a}_1+0\mathbf{a}_2-1\mathbf{a}_3+1\mathbf{a}_4++0\mathbf{a}_5+1\mathbf{a}_6+1\mathbf{a}_7=0
$$
把系数写出来：$\lambda=[0,0,-1,1,0,1,1]$，他就是对应上图中的绿色向量（相反方向）。因此，只有沿着这个方向走适合的步长$\theta$，就能到达下一个顶点。即新的顶点和旧的顶点关系为：
$$
\mathbf{x&amp;rsquo;}=\mathbf{x}-\theta\mathbf{\lambda}(\theta&amp;gt;0)
$$&lt;/p&gt;
&lt;p&gt;那么多大的$\theta$合适呢？我们很容易知道$\mathbf{x&amp;rsquo;}$能够满足$\mathbf{Ax=b}$，因为$\mathbf{A\lambda=0}$，现在要保证的就是$\mathbf{x&amp;rsquo;}$的各个分量大于等于0。对于$\lambda_i\leq0$的项，相减后大于0，没问题。但是对于$\lambda_i&amp;gt;0$的项，就要小心了，为了保证相减后仍然大于等于0，我们设置
$$
\theta=\min\limits_{\mathbf{a}_i\in \mathbf{B},\lambda_i&amp;gt;0}\frac{x_i}{\lambda_i}
$$&lt;/p&gt;
&lt;p&gt;就能保证$\mathbf{x&amp;rsquo;}\geq0$。在上面的例子中，$\theta=2$，新解是$\mathbf{x&amp;rsquo;}=[2,0,2,0,0,1,4]$，对应的基是$\mathbf{B&amp;rsquo;}=\{\mathbf{a}_1，\mathbf{a}_3，\mathbf{a}_6，\mathbf{a}_7\}$，到这里，一切看上去很完美，我们找到了运动到下一个顶点的方法，也就是先找一个非基向量，将他写成用基向量表示的形式，提出系数，确定步长，得到新解。但是还有一个小问题，我们看到实际上$\mathbf{B&amp;rsquo;}$和$\mathbf{B}$差了一个向量，相当于把$\mathbf{a_4}$换出去，把$\mathbf{a}_3$换进来。我们称这个过程为换基，后面算法实现部分叫pivot。那么，怎么保证换了个向量之后，仍然是基呢？证明一下：&lt;/p&gt;
&lt;p&gt;证明：$\mathbf{B&amp;rsquo;}=\mathbf{B}-{\mathbf{a}_l}+{\mathbf{a}_e}$仍然是基。（$l$表示leave，$e$表示enter）&lt;/p&gt;
&lt;p&gt;假设$\mathbf{B&amp;rsquo;}$线性相关，那么存在$&amp;lt;d_1,d_2,\ldots,d_{l-1},d_{l+1},\ldots,d_m, d_e&amp;gt; \neq 0$，使得$\sum_{k}d_k\mathbf{a}_k=0$。而$\mathbf{a}_e=\sum_{i=1}^m \lambda_i\mathbf{a_i}$，代入得：
$$
(d_1+d_e\lambda_1)\mathbf{a_1}+\ldots+(d_e\lambda_l)\mathbf{a}_l+\ldots+(d_m+d_e\lambda_m)\mathbf{a}_m=0
$$&lt;/p&gt;
&lt;p&gt;这里是证明的关键之处：我们在设置$\theta$时的做法，假如最终选出来的使得$\frac{x_i}{\lambda_i}$最小的那一项下标为$p$，那么得到的新解的第$p$项必然为0，相当于把$\mathbf{a}_p$换了出去。在上面这个例子中$p=4$。而因为我们只考虑$\lambda_i&amp;gt;0$的基向量，因此被换出去的基$\mathbf{a}_l$对应的$\lambda_l&amp;gt;0$，因此上式中有$d_1=d_2=\ldots=d_m=d_e=0$，和原假设矛盾，因此$\mathbf{B&amp;rsquo;}$也是线性无关的。&lt;/p&gt;
&lt;p&gt;截止到目前，我们回答了三个问题，基本上单纯形算法呼之欲出了，单纯形算法就是通过反复的基变换（通过向量的进出）来找顶点，从而找到达到最优值的顶点。但是还是有些细节需要琢磨，比如，怎么选入基向量？改善的过程什么时候停止？&lt;/p&gt;
&lt;h2 id=&#34;4-入基向量的选择及停止准则&#34;&gt;4 入基向量的选择及停止准则&lt;/h2&gt;
&lt;p&gt;以最小化问题为标准，我们的最终目标是最小化$\mathbf{c^Tx}$，因此一个很自然的贪心想法是每步的改善都尽可能地大，因此可以计算一下更新的目标函数值和原来的差值，取使得变化大的顶点继续下一步迭代。那么这个差值怎么能够向量化地计算呢？只有向量化地计算，才能避免一个一个地计算比较，提高效率。&lt;/p&gt;
&lt;p&gt;假设$\mathbf{B}=\{\mathbf{a}_1, \mathbf{a}_2,\ldots, \mathbf{a}_m\}​$，那么对于任何一个非基向量$\mathbf{a}_e​$，都有$\mathbf{a}_e=\lambda_1\mathbf{a}_1+\ldots+\lambda_m\mathbf{a}_m​$。将$\lambda​$写完整：$\lambda=[\lambda_1,\lambda_2,\ldots,\lambda_m,0,0,\ldots,-1,\ldots,0]^T​$，差值
$$
\mathbf{c}^T\mathbf{x}&#39;-\mathbf{c}^T\mathbf{x}=\mathbf{c}^T(-\theta\lambda)=\theta(\mathbf{c}_e-\sum_{\mathbf{a}_i\in B}\lambda_i\mathbf{c}_i)
$$&lt;/p&gt;
&lt;p&gt;因此我们要选使得这个值的绝对值最大的$\mathbf{a}_e​$。那么什么时候表示找到最优值应该停止呢？很明显，就是对于所有$\mathbf{a}_e​$，这个差值都大于等于0，即目标函数不再减小。因此，每次迭代，先计算差值，如果存在小于0的，就选一个使得差值绝对值最大的作为入基向量。&lt;/p&gt;
&lt;p&gt;嗯，接下来就是要考虑向量化操作。首先我们看一下$\lambda$能不能向量化表示：由于$\mathbf{B}\lambda=\mathbf{a}_e$（$\lambda$只取基系数部分），因此$\lambda=\mathbf{B^{-1}}\mathbf{a}_e$，如果对整个矩阵$\mathbf{A}$左乘$\mathbf{B^{-1}}$，这就很有意思了，所有的非基列将变成该非基向量用基向量表示的系数，而所有的基列将变成$e_k$，即合起来成为一个单位阵的形式。这是很关键的一步，在单纯形算法实现中也涉及到，先记下。进一步，我们取$\mathbf{c}$的基部分$\mathbf{c_B}$，这样$\mathbf{c}_\mathbf{B}^T\mathbf{B}^{-1}\mathbf{A}$就等于了上式中的$\sum*_{\mathbf{a}_i\in \mathbf{B}}\lambda_i\mathbf{c}_i$的向量化形式（对所有的非基向量一同操作）。然后再加上一部分，变成$\mathbf{c}^T-\mathbf{c}_\mathbf{B}^T\mathbf{B}^{-1}\mathbf{A}$，这就是最终的形式，称之为检验数$\bar{\mathbf{c}}$。很容易验证，基向量对应的检验数都是0，我们的目标就是通过迭代，使得$\bar{\mathbf{c}}\geq0$，这时对于任何一个可行解$\mathbf{y}$（$\mathbf{Ay=b,y\geq0}$），都有$\mathbf{c}^T\mathbf{y}\geq\mathbf{c}_\mathbf{B}^T\mathbf{B}^{-1}\mathbf{A}\mathbf{y}=\mathbf{c}_\mathbf{B}^T\mathbf{B}^{-1}\mathbf{b}=\mathbf{c}_\mathbf{B}^T\mathbf{x}_\mathbf{B}=\mathbf{c}^T\mathbf{x}$，即$\mathbf{x}$就是最优的。&lt;/p&gt;
&lt;h2 id=&#34;5-单纯性算法核心单纯形表&#34;&gt;5 单纯性算法核心：单纯形表&lt;/h2&gt;
&lt;p&gt;终于，一系列的讲解结束，单纯性算法也就顺理成章了。要将上面的一堆东西整理成一个简短高效的可行算法并不简单。先贴上算法伪代码：&lt;/p&gt;
&lt;div align=center&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/img_1518795226.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div align=center&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/img_1518795334.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;再献上一个非常漂亮的单纯形表：&lt;/p&gt;
&lt;div align=center&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/img_1518795443.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;现在，我们来对算法进行分析，将算法的每个操作和我们上面的介绍对应起来。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;SIMPLEX算法一开始调用INITIALIZESIMPLEX找到一个初始基可行解，这在某些情况下很简单，当$\mathbf{b}\geq0$时，他就是一个初始基可行解，否则，可能要用到两阶段法、大M法等求，这不是重点。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;WHILE循环内，只要找到一个$c_e&amp;lt;0$，就继续迭代。第10到16行就是通过设定$\theta$找到出基向量。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;最关键最有意思的就是PIVOT算法，他巧妙地将我们介绍的繁杂操作使用一个简单的高斯行变换就实现了。而这个算法的载体就是单纯形表，如上图，左上角是目标函数值相反数$-z$，第一行是检验数$\bar{\mathbf{c}}$，左下角是基对应的部分解（其他部分是0，不用写出），右下角是矩阵$\mathbf{A}$。他始终被分块成两部分，基向量部分始终以单位阵的形式存在。注意左边的部分解每个分量都是严格对应着一个基向量，即他们是有顺序的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一行一行地看PIVOT算法。输入参数告诉我们下标为$l​$的向量被换出，因此找到他对应的那行，暂称为第$l​$行，这一行对应的基的下标要被换成$e​$，那么为什么更新后对应的解是$\frac{b_l}{a_{le}}​$呢，要注意，其实这个值就是$\theta​$，$b_l​$就是旧的$x_i​$，$a_{le}​$就是$\lambda_i​$（上面已经解释了乘上$\mathbf{B^{-1}}​$后每一列都是系数）。那么为什么更新后是$\theta​$呢？我们回到式子$\mathbf{x&#39;=x-\theta\lambda}​$，由于$b_l​$现在对应的新向量不是$\mathbf{x}​$对应的基向量，因此$\mathbf{x}​$在该位置的值是0，而我们知道$\lambda​$在入基向量对应的位置的值是-1，因此$0-(-1)\theta=\theta​$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;第3到4行，将第$l$行除以$a_{le}$，目的就是将$a_{le}$变成1，因为要始终保持基是以单位阵的形式存在。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;第8行，就是在执行$\mathbf{x&#39;=x-\theta\lambda}$的操作，得到新解。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;第10行，高斯行变换，你会发现这样操作完后，入基列就变成和刚才出基列一样，高斯行变换保证了矩阵的性质。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;第14行，我们知道$-z=0-\mathbf{c}_\mathbf{B}^T\mathbf{B}^{-1}\mathbf{b}​$，由于旧有的基对应的$c​$都是0，而只有新换入的向量对应的$c_e​$不为0，具体写一下，减掉的那部分就只有$c_e​$和他对应的解$b_l​$的乘积了。同理，第16行，$\mathbf{c}^T-\mathbf{c}_\mathbf{B}^T\mathbf{B}^{-1}\mathbf{A}​$，由于也是只有$c_e​$不为0，因此就和他对应的$\mathbf{A}​$的第$l​$行相乘了。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;到此，终于介绍完了单纯形算法。其他还有一些要注意的地方，比如一定要注意检验数和原目标函数的$\mathbf{c}​$是完全不一样的概念，在原约束为不等式，需要加松弛变量的情况下，他们可能相等，但心里一定要区分它们，同时，这种情况下，基很容易找，就是松弛变量的那几列构成的单位阵。但是如果原约束是等式，就需要自己找基，并且这时检验数往往就和目标函数参数不同了。&lt;/p&gt;
&lt;p&gt;最后，本文所用的截图均来自中科院计算所B老师的课程PPT，本人在学习该课程时也受益良多，对单纯形算法也钻研了比较长的时间，因此撰写本文，希望给大家一个学习参考！其中可能有错误之处，欢迎指正讨论。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>递归神经网络</title>
      <link>https://kennethwong.tech/post/rnn/</link>
      <pubDate>Fri, 04 Jun 2021 18:12:26 +0800</pubDate>
      <guid>https://kennethwong.tech/post/rnn/</guid>
      <description>&lt;h2 id=&#34;1-rnn&#34;&gt;1 RNN&lt;/h2&gt;
&lt;p&gt;人在阅读文字时，具有短暂的记忆，比如在阅读当前词汇时你会对之前的文字有一些印象，而不是全部丢弃前面的内容。对于神经网络，我们也需要这样的功能，即保持对之前内容的短暂记忆。而RNN递归神经网络提供了一种实现方法。&lt;/p&gt;
&lt;p&gt;最简单的RNN递归公式如下（其中$\hat{y}^{(t)}$是预测的概率向量，$y^{(t)}$是标量类别标签）：
$$
\begin{equation}
\begin{split}
h^{(t)}&amp;amp;=\tanh(Ux^{(t)}+Wh^{(t-1)}+b) \cr
o^{(t)}&amp;amp;=Vh^{(t)}+c \cr
\hat{y}^{(t)}&amp;amp;=\mathrm{softmax}(o^{(t)})\cr
L^{(t)}&amp;amp;=-\log \hat{y}^{(t)}_{y_t}\cr
L&amp;amp;=\sum L^{(t)}
\end{split}
\end{equation}
$$
BPTT梯度回传算法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;​&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$$
\nabla_{o_i^{(t)}}L=\nabla_{o_i^{(t)}} L^{(t)}=-\nabla_{o_i^{(t)}}\log \frac{\exp(o_{y^{(t)}}^{(t)})}{\sum_j\exp(o_j^{(t)})}=\hat{y}_i^{(t)}-\mathbf{l}{i=y^{(t)}}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;向量表示：
$$
\nabla_{o^{(t)}}L=\hat{\mathbf{y}}^{(t)}-\mathbf{y}^{(t)}
$$
其中$\mathbf{y}^{(t)}$是由标量标签扩展的one-hot向量。即总的损失对任意一个时刻的输出的导数都可以表示成上述形式。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对最后一个时间$\tau$的导数：
$$
\nabla_{h^{(\tau)}}L=V^T\nabla_{o^{(\tau)}}L^{(t)}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于任意的$t&amp;lt;\tau$，$h^{(t)}$接收两个方向回传的梯度：$h^{(t+1)}$和$o^{(t)}$。
$$
\begin{align}\nabla_{h^{(t)}}L &amp;amp;=\left(\frac{\partial o^{(t)}}{\partial h^{(t)}}\right)^T\nabla_{o^{(t)}}L+\left(\frac{\partial h^{(t+1)}}{\partial h^{(t)}}\right)^T\nabla_{h^{(t+1)}}L \cr
&amp;amp;= V^T\nabla_{o^{(t)}}L + W^T\mathrm{diag}(1-(h^{(t+1)})^2)(\nabla_{h^{(t+1)}}L)\end{align}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对参数的求导，因为所有时间步共享参数，因此需要对所有时间步累加导数。
$$
\begin{equation}
\begin{split}
\nabla_c L &amp;amp;= \sum_t \nabla_{o^{(t)}} L \cr
\nabla_b L&amp;amp;=\sum_t \mathrm{diag}(1-(h^{(t)})^2)\nabla_{h^{(t)}}L \cr
\nabla_V L &amp;amp;= \sum_t (\nabla_{o^{(t)}}L) (h^{(t)})^T \cr
\nabla_W L &amp;amp;= \sum_t \mathrm{diag}(1-(h^{(t)})^2)(\nabla_{h^{(t)}}L) (h^{(t-1)})^T \cr
\nabla_U L &amp;amp;= \sum_t  \mathrm{diag}(1-(h^{(t)})^2)(\nabla_{h^{(t)}}L) (x^{(t)})^T
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;传统的RNN会有梯度消失问题，这是因为误差项累积相乘，也就是不能保持长期记忆。因此需要引入一些控制门缓解梯度消失，这样做会使得梯度误差项变成相加。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;​&lt;/p&gt;
&lt;h2 id=&#34;2-lstm&#34;&gt;2 LSTM&lt;/h2&gt;
&lt;p&gt;LSTM的提出是希望改善RNN缺少长期记忆的缺点。RNN当时间步达到一定长度时，模型很可能会忘掉之前的记忆。首先来看一张形象的LSTM模型图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/rnn/img_1521165195.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
 &lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/rnn/img_1521165219.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;首先，图中顶部贯穿的一条直线是每个胞元的内部状态$c_t$。三个sigmoid函数构成三个门：输入门，遗忘门，输出门。之所以叫“门”，是因为sigmoid能够将数值	归一化到（0，1）区间，当他和向量对应元素相乘时，能够决定这个向量的保留量。三个门的计算公式有相同的形式：
$$
\begin{equation}
\begin{split}
i&amp;amp;=\sigma(U_ix_t+W_ih_{t-1})\cr
f&amp;amp;=\sigma(U_fx_t+W_fh_{t-1})\cr
o&amp;amp;=\sigma(U_ox_t+W_oh_{t-1})
\end{split}
\end{equation}
$$
上述公式有不同写法，比如，如果合并成一个矩阵写也可以：
$$
i=\sigma(W_i[x_t, h_{t-1}]+b_i)
$$
这与上面的式子是等价的，相当于将上面的$U_i$和$W_i$在1维度合并。&lt;/p&gt;
&lt;p&gt;当从一个胞元过度到下一个时，首先要确定要忘记多少之前的状态，即使用遗忘门来控制过去状态$c_{t-1}$的权重。&lt;/p&gt;
&lt;p&gt;然后，在当前状态，我们获得的新的知识需要计算，这种新的状态、输出状态一般用tanh激活函数，输出范围（-1，1），那么新的状态部分可以计算如下：
$$
\bar{c}_t=\tanh(U_cx_t+W_ch_{t-1})
$$
新状态使用输入门控制输入量，结合上面的遗忘门控制的旧状态占比，可计算出新的胞元状态：
$$
c_t=f * c_{t-1}+i*\bar{c}_t
$$
最后计算这个胞元的输出态，用输出门控制信息量：
$$
h_t=o_t*\tanh(c_t)
$$&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/rnn/img_1521167732.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;RNN时LSTM的特殊情况：当输入门全1，遗忘门全0（总是抛弃过去的记忆），输出门全1时，就是一个RNN。&lt;/p&gt;
&lt;h2 id=&#34;3-gru&#34;&gt;3 GRU&lt;/h2&gt;
&lt;p&gt;GRU将LSTM的输入门和遗忘门合并成一个更新门$z$，并且将细胞状态和输出状态合并。&lt;/p&gt;
&lt;div align=center&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/rnn/img_1521168305.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;公式如下：
$$
\begin{equation}
\begin{split}
r_t&amp;amp;=\sigma(U_rx_t+W_rh_{t-1})\cr
z_t&amp;amp;=\sigma(U_zx_t+W_zh_{t-1})\cr
h_t&amp;amp;=z_t*h_{t-1}+(1-z_t)\tanh(U_hx_t+W_h(r_t*h_{t-1}))
\end{split}
\end{equation}
$$
​&lt;/p&gt;
&lt;p&gt;​&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
