<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>深度学习 | Kenneth</title>
    <link>/zh/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link>
      <atom:link href="/zh/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <description>深度学习</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>zh-Hans</language><copyright>© 2021 All rights reserved.</copyright><lastBuildDate>Sun, 27 Jun 2021 21:57:34 +0800</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>深度学习</title>
      <link>/zh/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link>
    </image>
    
    <item>
      <title>Swin Transformer 解析</title>
      <link>/zh/post/swin_transformer/</link>
      <pubDate>Sun, 27 Jun 2021 21:57:34 +0800</pubDate>
      <guid>/zh/post/swin_transformer/</guid>
      <description>&lt;h3 id=&#34;swin-transformer1&#34;&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2103.14030v1.pdf&#34; title=&#34;Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, arXiv 2021.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Swin-Transformer&lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;Transformer在NLP领域被广泛应用，能够建立大范围的数据之间的依赖关系，以attention的形式。语言具有良好的单个词作为元素基础，但是图像的基本元素在尺度变化上是非常大的，现有的基于transformer的视觉模型的token尺度都是固定的，对于视觉任务并不适合。另一方面，视觉任务具有更高的分辨率要求，比如semantic segmentation，精确到像素级，并不适合直接利用transformer的self attention机制（将是image size的平方了量级）。而Swin Transformer构建层级的feature map，而且计算复杂度是image size的线性函数。整体来说，Swin Transformer先由小patch开始，再到深层融合邻居patch信息。并且self-attention只用在不相邻的大窗口（就是shift window，Swin的全称）内，窗口之间并没有overlap。&lt;/p&gt;
&lt;div align=center&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/swin_transformer/swin.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Swin-Transformer作为一个适合与视觉任务，特别是适合稠密预测（检测、分割）的backbone而出现，它可以结合众多的检测方法。在此之前，有
&lt;a href=&#34;https://arxiv.org/pdf/2012.12877.pdf&#34; title=&#34;Training data-efficient image transformers and distillation through attention, 2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ViT&lt;/a&gt;
以及它的改进工作提出的backbone，ViT需要大量数据预训练（JFT-300M），它的改进[DeiT][3]运用了一些训练策略使得可以在ImageNet-1K上预训练。ViT虽然取得令人振奋的效果，但是它本身其实并不适合作为密集视觉任务的backbone，因为它的低分辨率的特征图与平方计算复杂度。&lt;/p&gt;
&lt;h4 id=&#34;细节过程&#34;&gt;细节过程&lt;/h4&gt;
&lt;div align=center&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/swin_transformer/swin_frwork.png&#34; alt=&#34;Swin Transformer基本结构&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;这里结合代码研究分析Swin Transformer的具体过程和原理。先给出整个流程的定义。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class SwinTransformer(nn.Module):
    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,
                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],
                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False, **kwargs):
        super().__init__()

        self.num_classes = num_classes 
        self.num_layers = len(depths)  ## 4
        self.embed_dim = embed_dim  # 96
        self.ape = ape  # the result is bad
        self.patch_norm = patch_norm  
        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))  # 96 * 2^3
        self.mlp_ratio = mlp_ratio

        # split image into non-overlapping patches
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,
            norm_layer=norm_layer if self.patch_norm else None)
        num_patches = self.patch_embed.num_patches  # 56*56
        patches_resolution = self.patch_embed.patches_resolution  # (56, 56)
        self.patches_resolution = patches_resolution

        # absolute position embedding
        if self.ape:
            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
            trunc_normal_(self.absolute_pos_embed, std=.02)

        self.pos_drop = nn.Dropout(p=drop_rate)

        # stochastic depth
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule

        # build layers
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),
                               input_resolution=(patches_resolution[0] // (2 ** i_layer),
                                                 patches_resolution[1] // (2 ** i_layer)),
                               depth=depths[i_layer],
                               num_heads=num_heads[i_layer],
                               window_size=window_size,
                               mlp_ratio=self.mlp_ratio,
                               qkv_bias=qkv_bias, qk_scale=qk_scale,
                               drop=drop_rate, attn_drop=attn_drop_rate,
                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
                               norm_layer=norm_layer,
                               downsample=PatchMerging if (i_layer &amp;lt; self.num_layers - 1) else None,
                               use_checkpoint=use_checkpoint)
            self.layers.append(layer)

        self.norm = norm_layer(self.num_features)
        self.avgpool = nn.AdaptiveAvgPool1d(1)
        self.head = nn.Linear(self.num_features, num_classes) if num_classes &amp;gt; 0 else nn.Identity()

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {&#39;absolute_pos_embed&#39;}

    @torch.jit.ignore
    def no_weight_decay_keywords(self):
        return {&#39;relative_position_bias_table&#39;}

    def forward_features(self, x):
        x = self.patch_embed(x)
        if self.ape:
            x = x + self.absolute_pos_embed
        x = self.pos_drop(x)

        for layer in self.layers:
            x = layer(x)

        x = self.norm(x)  # B L C
        x = self.avgpool(x.transpose(1, 2))  # B C 1
        x = torch.flatten(x, 1)
        return x

    def forward(self, x):
        x = self.forward_features(x)
        x = self.head(x)
        return x
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;预处理&#34;&gt;预处理&lt;/h5&gt;
&lt;p&gt;对于分类模型，输入图像尺寸为$224\times 224 \times 3$，即$H=W=224$。按照原文描述，模型先将图像分割成每块大小为$4\times 4$的patch，那么就会有$56\times 56$个patch，这就是初始resolution，也是后面每个stage会降采样的维度。后面每个stage都会降采样时长宽降到一半，特征数加倍。按照原文及原图描述，划分的每个patch具有$4\times4\times3=48$维特征。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;实际在代码中，首先使用了PatchEmbed模块，定义如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class PatchEmbed(nn.Module):
    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
        self.img_size = img_size
        self.patch_size = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1]
  
        self.in_chans = in_chans
        self.embed_dim = embed_dim
  
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None
  
    def forward(self, x):
        B, C, H, W = x.shape
        # FIXME look at relaxing size constraints
        assert H == self.img_size[0] and W == self.img_size[1], \
            f&amp;quot;Input image size ({H}*{W}) doesn&#39;t match model ({self.img_size[0]}*{self.img_size[1]}).&amp;quot;
        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C
        if self.norm is not None:
            x = self.norm(x)
        return x
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以看到，实际操作使用了一个卷积层conv2d(3, 96, 4, 4)，直接就做了划分patch和编码初始特征的工作，对于输入$x: B\times 3\times 224\times 224$，经过一层conv2d和LayerNorm得到$x: B\times 56^2\times 96$。然后作为对比，可以选择性地加上每个patch的绝对位置编码，原文实验表示这种做法不好，因此不会采用（ape=false）。最后经过一层dropout，至此，预处理完成。另外，要注意的是，代码和上面流程图并不符，其实在stage 1之前，即预处理完成后，维度已经是$H/4\times W/4\times C$，stage 1之后已经是$H/8\times W/8\times 2C$，不过在stage 4后不再降采样，得到的还是$H/32\times W/32 \times 8C$。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;stage处理&#34;&gt;stage处理&lt;/h5&gt;
&lt;p&gt;我们先梳理整个stage的大体过程，把简单的部分先说了，再深入到复杂得的细节。每个stage，即代码中的BasicLayer，由若干个block组成，而block的数目由depth列表中的元素决定。每个block就是W-MSA（window-multihead self attention）或者SW-MSA（shift window multihead self attention），一般有偶数个block，两种SA交替出现，比如6个block，0，2，4是W-MSA，1，3，5是SW-MSA。在经历完一个stage后，会进行下采样，定义的下采样比较有意思。比如还是$56\times 56$个patch，四个为一组，分别取每组中的左上，右上、左下、右下堆叠一起，经过一个layernorm，linear层，实现维度下采样、特征加倍的效果。实际上它可以看成一种加权池化的过程。代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class PatchMerging(nn.Module):
    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
        super().__init__()
        self.input_resolution = input_resolution
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = norm_layer(4 * dim)

    def forward(self, x):
        &amp;quot;&amp;quot;&amp;quot;
        x: B, H*W, C
        &amp;quot;&amp;quot;&amp;quot;
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, &amp;quot;input feature has wrong size&amp;quot;
        assert H % 2 == 0 and W % 2 == 0, f&amp;quot;x size ({H}*{W}) are not even.&amp;quot;

        x = x.view(B, H, W, C)

        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C

        x = self.norm(x)
        x = self.reduction(x)

        return x
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在经历完4个stage后，得到的是$(H/32\times W/32)\times 8C$的特征，将其转到$8C\times (H/32\times W/32)$后，接一个AdaptiveAvgPool1d(1)，全局平均池化，得到$8C$特征，最后接一个分类器。&lt;/p&gt;
&lt;h5 id=&#34;block处理&#34;&gt;Block处理&lt;/h5&gt;
&lt;p&gt;上面说到有两种block，block的代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class SwinTransformerBlock(nn.Module):
    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        if min(self.input_resolution) &amp;lt;= self.window_size:
            # if window size is larger than input resolution, we don&#39;t partition windows
            self.shift_size = 0
            self.window_size = min(self.input_resolution)
        assert 0 &amp;lt;= self.shift_size &amp;lt; self.window_size, &amp;quot;shift_size must in 0-window_size&amp;quot;

        self.norm1 = norm_layer(dim)
        self.attn = WindowAttention(
            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,
            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)

        self.drop_path = DropPath(drop_path) if drop_path &amp;gt; 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

        if self.shift_size &amp;gt; 0:
            # calculate attention mask for SW-MSA
            H, W = self.input_resolution
            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1
            h_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            w_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            cnt = 0
            for h in h_slices:
                for w in w_slices:
                    img_mask[:, h, w, :] = cnt
                    cnt += 1

            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1
            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
        else:
            attn_mask = None

        self.register_buffer(&amp;quot;attn_mask&amp;quot;, attn_mask)

    def forward(self, x):
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, &amp;quot;input feature has wrong size&amp;quot;

        shortcut = x
        x = self.norm1(x)
        x = x.view(B, H, W, C)

        # cyclic shift
        if self.shift_size &amp;gt; 0:
            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
        else:
            shifted_x = x

        # partition windows
        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C

        # W-MSA/SW-MSA
        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C

        # merge windows
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H&#39; W&#39; C

        # reverse cyclic shift
        if self.shift_size &amp;gt; 0:
            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
        else:
            x = shifted_x
        x = x.view(B, H * W, C)

        # FFN
        x = shortcut + self.drop_path(x)
        x = x + self.drop_path(self.mlp(self.norm2(x)))

        return x
&lt;/code&gt;&lt;/pre&gt;
&lt;h6 id=&#34;w-msa&#34;&gt;W-MSA&lt;/h6&gt;
&lt;p&gt;W-MSA比较简单，只要其中&lt;code&gt;shift_size&lt;/code&gt;设置为0就是W-MSA。下面跟着代码走一遍过程。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;输入：$x: B\times 56^2\times 96$，$H, W=56$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;经过一层layerNorm&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;变形：$x: B\times 56\times 56\times 96$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;直接赋值给&lt;code&gt;shifted_x&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;调用&lt;code&gt;window_partition&lt;/code&gt;函数，输入&lt;code&gt;shifted_x&lt;/code&gt;，&lt;code&gt;window_size=7&lt;/code&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;注意窗口大小以patch为单位，比如7就是7个patch，如果56的分辨率就会有8个窗口。&lt;/li&gt;
&lt;li&gt;这个函数对&lt;code&gt;shifted_x&lt;/code&gt;做一系列变形，最终变成$8^2B\times 7 \times 7\times 96$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;返回赋值给&lt;code&gt;x_windows&lt;/code&gt;，再变形成$8^2B\times 7^2\times 96$，这表示所有图片，每个图片的64个window，每个window内有49个patch。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;调用&lt;code&gt;WindowAttention&lt;/code&gt;层，这里以它的&lt;code&gt;num_head&lt;/code&gt;为3为例。输入参数为&lt;code&gt;x_windows&lt;/code&gt;和&lt;code&gt;self.attn_mask&lt;/code&gt;，对于W-MSA，&lt;code&gt;attn_mask&lt;/code&gt;为None，可以不用管。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;WindowAttention&lt;/code&gt;代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class WindowAttention(nn.Module):
    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):
    
        super().__init__()
        self.dim = dim
        self.window_size = window_size  # Wh, Ww
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5
    
        # define a parameter table of relative position bias
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH
    
        # get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
        self.register_buffer(&amp;quot;relative_position_index&amp;quot;, relative_position_index)
    
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
    
        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)
    
    def forward(self, x, mask=None):
        &amp;quot;&amp;quot;&amp;quot;
        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
        &amp;quot;&amp;quot;&amp;quot;
        B_, N, C = x.shape
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)
    
        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))
    
        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
        attn = attn + relative_position_bias.unsqueeze(0)
    
        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)
    
        attn = self.attn_drop(attn)
    
        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;输入$x: 8^2B\times 7^2\times 96$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;产生$QKV$，调用线性层后，得到$8^2B\times 7^2\times (96\times 3)$，拆分给不同的head，得到$8^2B\times 7^2\times 3 \times 3\times 32$，第一个3是$QKV$的3，第二个3是3个head。再permute成$3\times 8^2B\times 3\times 7^2\times 32$，再拆解成$q,k,v$，每个都是$8^2B\times 3\times 7^2\times 32$。表示所有图片的每个图片64个window，每个window对应到3个不同的head，都有一套49个patch、32维的特征。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$q$归一化&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$qk$矩阵相乘求特征内积，得到$attn: 8^2B \times 3\times 7^2\times 7^2$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;得到相对位置的编码信息&lt;code&gt;relative_position_bias&lt;/code&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH
      
# get pair-wise relative position index for each token inside the window
coords_h = torch.arange(self.window_size[0])
coords_w = torch.arange(self.window_size[1])
coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
relative_coords[:, :, 1] += self.window_size[1] - 1
relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
self.register_buffer(&amp;quot;relative_position_index&amp;quot;, relative_position_index)
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这里以&lt;code&gt;window_size=3&lt;/code&gt;为例，解释以下过程：首先生成$coords: 2\times3\times 3$，就是在一个$3\times 3$的窗口内，每个位置的$y,x$坐标，而&lt;code&gt;relative_coords&lt;/code&gt;为$2\times 9 \times 9$，就是9个点中，每个点的$y$或$x$与其他所有点的差值，比如$[0]
&lt;a href=&#34;https://arxiv.org/pdf/2103.14030v1.pdf&#34; title=&#34;Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, arXiv 2021.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3&lt;/a&gt;
$，表示3号点（第二行第一个点）与1号点（第一行第二个点）的$y$坐标的差值。然后变形，并让两个坐标分别加上$3-1=2$，是因为这些坐标值范围$[0,2]$，因此差值的最小值为-2，加上2后从0开始。最后让$y$坐标乘上$2\times 3-1=5$，应该是一个trick，调整差值范围。最后将两个维度的差值相加，得到&lt;code&gt;relative_position_index&lt;/code&gt;，$3^2\times 3^2$，为9个点之间两两之间的相对位置编码值，最后用来到&lt;code&gt;self.relative_position_bias_table&lt;/code&gt;中寻址，注意相对位置的最大值为$(2M-2)(2M-1)$，而这个table最多有$(2M-1)(2M-1)$行，因此保证可以寻址，得到了一组给多个head使用的相对位置编码信息，这个table是可训练的参数。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;回到代码中，得到的&lt;code&gt;relative_position_bias&lt;/code&gt;为$3\times 7^2\times7^2$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将其加到&lt;code&gt;attn&lt;/code&gt;上，最后一个维度softmax，dropout&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;与$v$矩阵相乘，并转置，合并多个头的信息，得到$8^2B\times 7^2\times 96$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;经过一层线性层，dropout，返回&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;返回赋值给&lt;code&gt;attn_windows&lt;/code&gt;，变形为$8^2B\times 7\times 7 \times 96$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;调用&lt;code&gt;window_reverse&lt;/code&gt;，打回原状：$B\times 56\times 56\times96$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;返回给$x$，经过FFN：先加上原来的输入$x$作为residue结构，注意这里用到[timm][https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/drop.py]的&lt;code&gt;DropPath&lt;/code&gt;，并且drop的概率是整个网络结构线性增长的。然后再加上两层mlp的结果。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;返回结果$x$。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这样，整个过程就完成了，剩下的就是SW-MSA的一些不同的操作。&lt;/p&gt;
&lt;h6 id=&#34;sw-msa&#34;&gt;SW-MSA&lt;/h6&gt;
&lt;p&gt;W-MSA的不足就不同window之间是没有交流的，因此，SW-MSA紧接在W-MSA后面就是想让不同window之间也有信息传递。因此，最直接的想法就是将图片平移以下，但不是平移&lt;code&gt;window_size&lt;/code&gt;，比如平移一半，再以跟之前相同的划分来切割，这样就会有上一轮本来不在同一个window里面的pach，现在出现在同一个window。但是，最简单的处理，比如像第一个图，会出现更多的window，有些window尺寸还不一样。一种修补方法是补上一些块，padding，但是会增大计算量。为此，这里采用cycling（rolling）的方法，即滚动，将图片向左、向上平移window一半的尺寸，比如window是7，这里就平移三个patch，左边的那些块移到右边，上边的那些块移到下边。&lt;/p&gt;
&lt;p&gt;再回到block的代码中，这时&lt;code&gt;shift_size=7//2=3&lt;/code&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;输入：$x: B\times 56^2\times 96$，$H, W=56$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;经过一层layerNorm&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;变形：$x: B\times 56\times 56\times 96$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对$x$进行roll：就是分别向左向上滚动，把块补在右面和下面，赋值给&lt;code&gt;shifted_x&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;调用&lt;code&gt;window_partition&lt;/code&gt;函数，输入&lt;code&gt;shifted_x&lt;/code&gt;，&lt;code&gt;window_size=7&lt;/code&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;注意窗口大小以patch为单位，比如7就是7个patch，如果56的分辨率就会有8个窗口。&lt;/li&gt;
&lt;li&gt;这个函数对&lt;code&gt;shifted\_x&lt;/code&gt;做一系列变形，最终变成$8^2B\times 7 \times 7\times 96$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;返回赋值给&lt;code&gt;x\_windows&lt;/code&gt;，再变形成$8^2B\times 7^2\times 96$，这表示所有图片，每个图片的64个window，每个window内有49个patch。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;调用&lt;code&gt;WindowAttention&lt;/code&gt;层，这里以它的&lt;code&gt;num_head&lt;/code&gt;为3为例。输入参数为&lt;code&gt;x_windows&lt;/code&gt;和&lt;code&gt;self.attn_mask&lt;/code&gt;，对于SW-MSA，&lt;code&gt;attn_mask&lt;/code&gt;的产生过程如下。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;mask的形状为$1\times56\times56\times 1$，产生&lt;code&gt;h_slices&lt;/code&gt;和&lt;code&gt;w_slices&lt;/code&gt;都是三个：(0, -7)，(-7, -3)，(-3, None)，通过一个循环，对mask分成9大块，并打上标号。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;下图为了方便显示，换了个例子：$12\times 12$个patch，window为$3\times 3$个patch，平移量1，$4\times4$个窗口&lt;/p&gt;
&lt;div align=center&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/swin_transformer/swin_mask.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;解释：经过循环，mask被分成9大部分，分别表上0到8，然后经过&lt;code&gt;window\_partition&lt;/code&gt;，得到$4^2\times3^2$，再作差，得到$4^2\times 3^2\times3^2$，将不为0的置为-100，将为0的填上0。可以理解成，在$4^2$个窗口中的每个窗口，都有9个位置，那么这9个位置中的每个应该关注哪些位置（0），不用关注哪些位置（-100），比如右上角那个区域的左上角位置，它对应的mask就应该是上面那一个横条，不用关注右边蓝色的三格，因为右边那列是由左边平移来的，而计算两个边缘之间相关性作用不大。其他位置同理。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;那么回到原来的代码中，产生的mask就应该是$8^2\times 7^2\times7^2$，计算attention时，将$attn: 8^2B \times 3\times 7^2\times 7^2$变形成$B\times 8^2\times 3\times7^2\times7^2$加上mask，再变回去，然后softmax。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这里其实存在一点疑问：因为窗口天然隔离，上面的(0,-7), (-7, -3)这一段分割显得多余。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;后面就和W-MSA一样了，不过还要把$x$转回去，否则就要不停地转动了。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最后，Swin-Transformer有4种架构：&lt;/p&gt;
&lt;div align=center&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/swin_transformer/swin_param.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;关于MSA中的复杂度运算：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于普通的MSA，在全图上计算SA，如果尺寸为$h\times w$，那么产生$QWV$的$W$矩阵为$C\times C$，输入$x$为$hw\times C$，因此需要复杂度$3hwC^2$。然后，计算$QK^T$，$QKV$的维度是$hw\times C$，因此为$(hw)^2C$。然后softmax乘$V$得到$Z$，复杂度为$(hw)^2C$。最后，从$Z$到输出，还要乘一个矩阵$W$，复杂度$hwC^2$，因此总复杂度为$4hwC^2+2(hw)^2C$。&lt;/li&gt;
&lt;li&gt;对于W-MSA，因为分窗口进行SA，因此主要优化的是$(hw)^2C$项，现在变成$W^2\times W^2\times (h/W)\times(w/W)C=W^2hwC$，其实还是四次的，只不过将其中一个$hw$变成可以更小的$W^2$，减小的部分就是窗口与窗口之间的信息传递。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>递归神经网络</title>
      <link>/zh/post/rnn/</link>
      <pubDate>Fri, 04 Jun 2021 18:12:26 +0800</pubDate>
      <guid>/zh/post/rnn/</guid>
      <description>&lt;h2 id=&#34;1-rnn&#34;&gt;1 RNN&lt;/h2&gt;
&lt;p&gt;人在阅读文字时，具有短暂的记忆，比如在阅读当前词汇时你会对之前的文字有一些印象，而不是全部丢弃前面的内容。对于神经网络，我们也需要这样的功能，即保持对之前内容的短暂记忆。而RNN递归神经网络提供了一种实现方法。&lt;/p&gt;
&lt;p&gt;最简单的RNN递归公式如下（其中$\hat{y}^{(t)}$是预测的概率向量，$y^{(t)}$是标量类别标签）：
$$
\begin{equation}
\begin{split}
h^{(t)}&amp;amp;=\tanh(Ux^{(t)}+Wh^{(t-1)}+b) \cr
o^{(t)}&amp;amp;=Vh^{(t)}+c \cr
\hat{y}^{(t)}&amp;amp;=\mathrm{softmax}(o^{(t)})\cr
L^{(t)}&amp;amp;=-\log \hat{y}^{(t)}_{y_t}\cr
L&amp;amp;=\sum L^{(t)}
\end{split}
\end{equation}
$$
BPTT梯度回传算法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;​&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$$
\nabla_{o_i^{(t)}}L=\nabla_{o_i^{(t)}} L^{(t)}=-\nabla_{o_i^{(t)}}\log \frac{\exp(o_{y^{(t)}}^{(t)})}{\sum_j\exp(o_j^{(t)})}=\hat{y}_i^{(t)}-\mathbf{l}{i=y^{(t)}}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;向量表示：
$$
\nabla_{o^{(t)}}L=\hat{\mathbf{y}}^{(t)}-\mathbf{y}^{(t)}
$$
其中$\mathbf{y}^{(t)}$是由标量标签扩展的one-hot向量。即总的损失对任意一个时刻的输出的导数都可以表示成上述形式。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对最后一个时间$\tau$的导数：
$$
\nabla_{h^{(\tau)}}L=V^T\nabla_{o^{(\tau)}}L^{(t)}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于任意的$t&amp;lt;\tau$，$h^{(t)}$接收两个方向回传的梯度：$h^{(t+1)}$和$o^{(t)}$。
$$
\begin{align}\nabla_{h^{(t)}}L &amp;amp;=\left(\frac{\partial o^{(t)}}{\partial h^{(t)}}\right)^T\nabla_{o^{(t)}}L+\left(\frac{\partial h^{(t+1)}}{\partial h^{(t)}}\right)^T\nabla_{h^{(t+1)}}L \cr
&amp;amp;= V^T\nabla_{o^{(t)}}L + W^T\mathrm{diag}(1-(h^{(t+1)})^2)(\nabla_{h^{(t+1)}}L)\end{align}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对参数的求导，因为所有时间步共享参数，因此需要对所有时间步累加导数。
$$
\begin{equation}
\begin{split}
\nabla_c L &amp;amp;= \sum_t \nabla_{o^{(t)}} L \cr
\nabla_b L&amp;amp;=\sum_t \mathrm{diag}(1-(h^{(t)})^2)\nabla_{h^{(t)}}L \cr
\nabla_V L &amp;amp;= \sum_t (\nabla_{o^{(t)}}L) (h^{(t)})^T \cr
\nabla_W L &amp;amp;= \sum_t \mathrm{diag}(1-(h^{(t)})^2)(\nabla_{h^{(t)}}L) (h^{(t-1)})^T \cr
\nabla_U L &amp;amp;= \sum_t  \mathrm{diag}(1-(h^{(t)})^2)(\nabla_{h^{(t)}}L) (x^{(t)})^T
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;传统的RNN会有梯度消失问题，这是因为误差项累积相乘，也就是不能保持长期记忆。因此需要引入一些控制门缓解梯度消失，这样做会使得梯度误差项变成相加。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;​&lt;/p&gt;
&lt;h2 id=&#34;2-lstm&#34;&gt;2 LSTM&lt;/h2&gt;
&lt;p&gt;LSTM的提出是希望改善RNN缺少长期记忆的缺点。RNN当时间步达到一定长度时，模型很可能会忘掉之前的记忆。首先来看一张形象的LSTM模型图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/rnn/img_1521165195.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
 &lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/rnn/img_1521165219.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;​	&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;首先，图中顶部贯穿的一条直线是每个胞元的内部状态$c_t$。三个sigmoid函数构成三个门：输入门，遗忘门，输出门。之所以叫“门”，是因为sigmoid能够将数值	归一化到（0，1）区间，当他和向量对应元素相乘时，能够决定这个向量的保留量。三个门的计算公式有相同的形式：
$$
\begin{equation}
\begin{split}
i&amp;amp;=\sigma(U_ix_t+W_ih_{t-1})\cr
f&amp;amp;=\sigma(U_fx_t+W_fh_{t-1})\cr
o&amp;amp;=\sigma(U_ox_t+W_oh_{t-1})
\end{split}
\end{equation}
$$
上述公式有不同写法，比如，如果合并成一个矩阵写也可以：
$$
i=\sigma(W_i[x_t, h_{t-1}]+b_i)
$$
这与上面的式子是等价的，相当于将上面的$U_i$和$W_i$在1维度合并。&lt;/p&gt;
&lt;p&gt;当从一个胞元过度到下一个时，首先要确定要忘记多少之前的状态，即使用遗忘门来控制过去状态$c_{t-1}$的权重。&lt;/p&gt;
&lt;p&gt;然后，在当前状态，我们获得的新的知识需要计算，这种新的状态、输出状态一般用tanh激活函数，输出范围（-1，1），那么新的状态部分可以计算如下：
$$
\bar{c}_t=\tanh(U_cx_t+W_ch_{t-1})
$$
新状态使用输入门控制输入量，结合上面的遗忘门控制的旧状态占比，可计算出新的胞元状态：
$$
c_t=f * c_{t-1}+i*\bar{c}_t
$$
最后计算这个胞元的输出态，用输出门控制信息量：
$$
h_t=o_t*\tanh(c_t)
$$&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/rnn/img_1521167732.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;RNN时LSTM的特殊情况：当输入门全1，遗忘门全0（总是抛弃过去的记忆），输出门全1时，就是一个RNN。&lt;/p&gt;
&lt;h2 id=&#34;3-gru&#34;&gt;3 GRU&lt;/h2&gt;
&lt;p&gt;GRU将LSTM的输入门和遗忘门合并成一个更新门$z$，并且将细胞状态和输出状态合并。&lt;/p&gt;
&lt;div align=center&gt;
&lt;p&gt;&lt;img src=&#34;https://wwb1-1258288365.cos.ap-beijing.myqcloud.com/rnn/img_1521168305.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;公式如下：
$$
\begin{equation}
\begin{split}
r_t&amp;amp;=\sigma(U_rx_t+W_rh_{t-1})\cr
z_t&amp;amp;=\sigma(U_zx_t+W_zh_{t-1})\cr
h_t&amp;amp;=z_t*h_{t-1}+(1-z_t)\tanh(U_hx_t+W_h(r_t*h_{t-1}))
\end{split}
\end{equation}
$$
​&lt;/p&gt;
&lt;p&gt;​&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
